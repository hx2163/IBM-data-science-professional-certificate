# Data Analysis with Python

## Cheat Sheet: Model Development

| Process | Description | Code Example |
|---------|-------------|--------------|
| Linear Regression | Create a Linear Regression model object | ```python\nfrom sklearn.linear_model import LinearRegression \n lr = LinearRegression()``` |
| Train Linear Regression model | Train the Linear Regression model on dedicated data, separating Input and Output attributes. When there is a single attribute in Input, then it is simple linear regression. When there are multiple attributes, it is multiple linear regression. | ```python\nX = df[['attribute_1', 'attribute_2', ...]] \n y = df['target_attribute'] \n lr.fit(X,y)``` |
| Generate output predictions | Predict the output for a set of input attribute values. | ```python\ny_hat = lr.predict(X)``` |
| Identify the coefficient and intercept | Identify the slope coefficient and intercept values of the linear regression model defined by \(y = mX + c\). Where m is the slope coefficient and c is the intercept. | ```python\ncoeff = lr.coef_ \n intercept = lr.intercept_``` |
| Residual Plot | This function will regress y on x (possibly as a robust or polynomial regression) and then draw a scatter plot of the residuals. | ```python\nimport seaborn as sns \n sns.residplot(df[['attribute_1']], y=df[['attribute_2']])``` |
| Distribution Plot | This function can be used to plot the distribution of data w.r.t. a given attribute. | ```python\nimport seaborn as sns \n sns.distplot(df['attribute_name'], hist=False) \n # can include other parameters like color, label and so on.``` |
| Polynomial Regression | Available under the numpy package, for single variable feature creation and model fitting. | ```python\nf = np.polyfit(x, y, n) \n # Generates the polynomial features of order n \n p = np.poly1d(f) \n y_hat = p(x) \n # p matches the polynomial model used to generate the predicted output \n y_hat is the predicted output``` |
| Multi-variate Polynomial Regression | Generates a new feature matrix consisting of all polynomial combinations of the features with the degree less than or equal to the specified degree. | ```python\nfrom sklearn.preprocessing import PolynomialFeatures \n Z = df[['attribute_1', 'attribute_2', ...]] \n poly = PolynomialFeatures(degree=2) \n Z_ = poly.fit_transform(Z)``` |
| Pipeline | Data Pipelines simplify the steps of processing the data. We create the pipeline by creating a list of tuples including the name of the model or estimator and its corresponding constructor. | ```python\nfrom sklearn.pipeline import Pipeline \n from sklearn.preprocessing import StandardScaler \n pipeline = Pipeline([('scaler', StandardScaler()), ('poly', PolynomialFeatures(include_bias=False)), ('model', LinearRegression())]) \n pipeline.fit(X,y) \n Z = pipeline.predict(X)``` |
| R² value | R², also known as the coefficient of determination, is a measure to indicate how close the data is to the fitted regression line. The value of the R²-squared is the percentage of variation of the response variable that is explained by a linear model. \n a. For Linear Regression (single or multi attribute) \n b. For Polynomial regression (single or multi attribute) | a. ```python\nX = df[['attribute_1', 'attribute_2', ...]] \n y = df['target_attribute'] \n lr = LinearRegression() \n lr.fit(X,y) \n R2_score = lr.score(X,y)``` \n b. ```python\nfrom sklearn.metrics import r2_score \n f = np.polyfit(x, y, n) \n p = np.poly1d(f) \n y_hat = p(x) \n R2_score = r2_score(y, y_hat)``` |
| MSE value | The Mean Squared Error measures the average of the squares of errors, that is, the difference between actual value and the estimated value. | ```python\nfrom sklearn.metrics import mean_squared_error \n mse = mean_squared_error(y, y_hat)``` |
